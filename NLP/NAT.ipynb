{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "path_to_kaggle_dataset = r'C:\\Users\\21022289\\Desktop\\C290\\unigram_freq.csv'\n",
    "kaggle_words = {}\n",
    "with open(path_to_kaggle_dataset, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader) \n",
    "    for row in csv_reader:\n",
    "        word, frequency = row\n",
    "        kaggle_words[word] = int(frequency)\n",
    "\n",
    "class SpellCorrector:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.WORDS = kaggle_words\n",
    "        self.N = sum(self.WORDS.values())\n",
    "        self.REGEX_TOKEN = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "    def tokens(self, text):\n",
    "        return self.REGEX_TOKEN.findall(text.lower())\n",
    "\n",
    "    def P(self, word):\n",
    "        return self.WORDS[word] / self.N\n",
    "\n",
    "    def most_probable(self, words):\n",
    "        _known = self.known(words)\n",
    "        if _known:\n",
    "            return max(_known, key=self.P)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def edit_step(word):\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        return (e2 for e1 in self.edit_step(word)\n",
    "                for e2 in self.edit_step(e1))\n",
    "\n",
    "    def known(self, words):\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def edit_candidates(self, word, assume_wrong=False, fast=True):\n",
    "\n",
    "        if fast:\n",
    "            ttt = self.known(self.edit_step(word)) or {word}\n",
    "        else:\n",
    "            ttt = self.known(self.edit_step(word)) or self.known(self.edits2(word)) or {word}\n",
    "\n",
    "        ttt = self.known([word]) | ttt\n",
    "        return list(ttt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, TFRobertaModel, RobertaForMaskedLM\n",
    "import pyttsx3\n",
    "from fastpunct import FastPunct\n",
    "import torch\n",
    "from math import log\n",
    "\n",
    "words = open(\"word_freq copy.txt\").read().split()\n",
    "wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))\n",
    "maxword = max(len(x) for x in words)\n",
    "\n",
    "word_list=[\"which\",\"our\",\"the\",\"of\",\"to\",\"and\",\"a\",\"in\",\"is\",\"it\",\"was\",\"for\",\"will\",\"be\",\"am\",\"on\",\"with\",\"an\",\"have\",\"its\",\".\",\"?\",\",\",\"!\"]\n",
    "\n",
    "config = 'roberta-large'\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-large')\n",
    "fastpunct = FastPunct() \n",
    "engine = pyttsx3.init()\n",
    "corrector = SpellCorrector()\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.roberta = TFRobertaModel.from_pretrained(config, from_pt=True)\n",
    "        self.roberta_config = self.roberta.config\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        output_layer = self.roberta(input_ids)[0]\n",
    "        embedding = self.roberta.get_input_embeddings()\n",
    "\n",
    "        with tf.name_scope('cls/predictions'):\n",
    "            with tf.name_scope('transform'):\n",
    "                input_tensor = tf.keras.layers.Dense(\n",
    "                    units=self.roberta_config.hidden_size, \n",
    "                    activation=self.roberta_config.hidden_act,\n",
    "                    kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.roberta_config.initializer_range),\n",
    "                )(output_layer)\n",
    "                input_tensor = tf.keras.layers.LayerNormalization()(input_tensor)\n",
    "\n",
    "            output_bias = tf.Variable(tf.zeros([self.roberta_config.vocab_size], dtype=tf.float32), name='output_bias')\n",
    "            logits = tf.matmul(input_tensor, embedding.weights[0], transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            return logits\n",
    "\n",
    "model1 = Model()\n",
    "\n",
    "def convert_to_speech(sentence):\n",
    "    engine.say(sentence)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def tokens_to_masked_ids(tokens, mask_ind):\n",
    "    masked_tokens = tokens[:]\n",
    "    masked_tokens[mask_ind] = \"[MASK]\"\n",
    "    masked_tokens = [\"[CLS]\"] + masked_tokens + [\"[SEP]\"]\n",
    "    masked_ids = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "    return masked_ids\n",
    "\n",
    "def get_score(mask):\n",
    "    tokens = tokenizer.tokenize(mask)\n",
    "    input_ids = [tokens_to_masked_ids(tokens, i) for i in range(len(tokens))]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, padding='post')\n",
    "    preds = tf.nn.softmax(model1(input_ids))\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return np.prod([preds[i, i + 1, x] for i, x in enumerate(tokens_ids)])\n",
    "\n",
    "def infer_spaces(s):\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i-maxword):i]))\n",
    "        return min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)\n",
    "\n",
    "    cost = [0]\n",
    "    for i in range(1,len(s)+1):\n",
    "        c,k = best_match(i)\n",
    "        cost.append(c)\n",
    "\n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i>0:\n",
    "        c,k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i-k:i])\n",
    "        i -= k\n",
    "\n",
    "    return \" \".join(reversed(out))\n",
    "\n",
    "def get_prediction(sent):\n",
    "    token_ids = tokenizer.encode(sent, return_tensors='pt')\n",
    "    masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "    masked_pos = [mask.item() for mask in masked_position ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(token_ids)\n",
    "\n",
    "    last_hidden_state = output[0].squeeze()\n",
    "\n",
    "    list_of_list =[]\n",
    "    for index,mask_index in enumerate(masked_pos):\n",
    "        mask_hidden_state = last_hidden_state[mask_index]\n",
    "        idx = torch.topk(mask_hidden_state, k=5, dim=0)[1]\n",
    "        words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
    "        list_of_list.append(words)\n",
    "    \n",
    "    best_guess = []\n",
    "    for j in list_of_list:\n",
    "        best_guess.append(j[0])\n",
    "        \n",
    "    return best_guess\n",
    "\n",
    "def update_sentence(text):\n",
    "    words = text.split()\n",
    "    replaced_words = []\n",
    "    updated_sentence = text\n",
    "    for word in words:\n",
    "        text_mask = text.replace(word, '[MASK]')\n",
    "        possible_states = corrector.edit_candidates(word)\n",
    "        scores = []\n",
    "        prob_scores = 0.0\n",
    "        if word not in possible_states:\n",
    "            replaced_masks = [text_mask.replace('[MASK]', state) for state in possible_states]\n",
    "\n",
    "            scores = [get_score(mask) for mask in replaced_masks]\n",
    "            print(scores)\n",
    "\n",
    "            prob_scores = np.array(scores)/np.sum(scores)\n",
    "\n",
    "            probs = list(zip(possible_states, prob_scores))\n",
    "            probs.sort(key=lambda x: x[1])\n",
    "            print(probs)\n",
    "\n",
    "            best_word, _ = probs[-1]\n",
    "            replaced_words.append(best_word)\n",
    "            updated_sentence = updated_sentence.replace(word, best_word)\n",
    "    \n",
    "    return updated_sentence\n",
    "\n",
    "def insert_words(text):\n",
    "    words = text.split()\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        output = \" \".join(words[:i+1] + [\"_\"] + words[i+1:])\n",
    "        masked_output = output.replace(\"_\",\"<mask>\")\n",
    "        predicted_blanks = get_prediction(masked_output)\n",
    "        for word in predicted_blanks:\n",
    "            if word in word_list:\n",
    "                predictions.append([word, i])\n",
    "\n",
    "    first_iteration=False\n",
    "    for i, l in enumerate(predictions,1):\n",
    "        word, index = l\n",
    "        words.insert(index+i, word)\n",
    "    correct_text=(' '.join(words))\n",
    "    return correct_text\n",
    "\n",
    "def correct_sentence(text):\n",
    "    spaced_text = infer_spaces(text)\n",
    "    updated_text = update_sentence(spaced_text)\n",
    "    correct_text = insert_words(updated_text)\n",
    "    correct_text_with_punct = fastpunct.punct(correct_text)\n",
    "    print(correct_text_with_punct)\n",
    "    convert_to_speech(correct_text_with_punct)\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.roberta = TFRobertaModel.from_pretrained(config, from_pt=True)\n",
    "        self.roberta_config = self.roberta.config\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        output_layer = self.roberta(input_ids)[0]\n",
    "        embedding = self.roberta.get_input_embeddings()\n",
    "\n",
    "        with tf.name_scope('cls/predictions'):\n",
    "            with tf.name_scope('transform'):\n",
    "                input_tensor = tf.keras.layers.Dense(\n",
    "                    units=self.roberta_config.hidden_size, \n",
    "                    activation=self.roberta_config.hidden_act,\n",
    "                    kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.roberta_config.initializer_range),\n",
    "                )(output_layer)\n",
    "                input_tensor = tf.keras.layers.LayerNormalization()(input_tensor)\n",
    "\n",
    "            output_bias = tf.Variable(tf.zeros([self.roberta_config.vocab_size], dtype=tf.float32), name='output_bias')\n",
    "            logits = tf.matmul(input_tensor, embedding.weights[0], transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            return logits\n",
    "\n",
    "model1 = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n",
      "[('proposal', nan)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-db71fc42a057>:101: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_scores = np.array(scores)/np.sum(scores)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning, how are you all? yesterday was I was able to create api for this application which I think is very useful for the business proposal.\n"
     ]
    }
   ],
   "source": [
    "text = \"goodmorninghowareyouallyesterdayiablecreateapithisapplicationithinkveryusefulforthebusinesspooposal\"\n",
    "correct_sentence(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
